{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dc160f60",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "papermill": {
          "duration": 0.009055,
          "end_time": "2023-10-27T20:56:19.591480",
          "exception": false,
          "start_time": "2023-10-27T20:56:19.582425",
          "status": "completed"
        },
        "tags": [],
        "id": "dc160f60"
      },
      "source": [
        "## Objective\n",
        "\n",
        "Use Llama 2.0, Langchain and ChromaDB to create a Retrieval Augmented Generation (RAG) system. This will allow us to ask questions about our documents (that were not included in the training data), without fine-tunning the Large Language Model (LLM).\n",
        "When using RAG, if you are given a question, you first do a retrieval step to fetch any relevant documents from a special database, a vector database where these documents were indexed.\n",
        "\n",
        "## Definitions\n",
        "\n",
        "* LLM - Large Language Model  \n",
        "* Llama 2.0 - LLM from Meta\n",
        "* Langchain - a framework designed to simplify the creation of applications using LLMs\n",
        "* Vector database - a database that organizes data through high-dimmensional vectors  \n",
        "* ChromaDB - vector database  \n",
        "* RAG - Retrieval Augmented Generation (see below more details about RAGs)\n",
        "\n",
        "## Model details\n",
        "\n",
        "* **Model**: Llama 2  \n",
        "* **Variation**: 7b-chat-hf  (7b: 7B dimm. hf: HuggingFace build)\n",
        "* **Version**: V1  \n",
        "* **Framework**: PyTorch  \n",
        "\n",
        "LlaMA 2 model is pretrained and fine-tuned with 2 Trillion tokens and 7 to 70 Billion parameters which makes it one of the powerful open source models. It is a highly improvement over LlaMA 1 model.\n",
        "\n",
        "\n",
        "## What is a Retrieval Augmented Generation (RAG) system?\n",
        "\n",
        "Large Language Models (LLMs) has proven their ability to understand context and provide accurate answers to various NLP tasks, including summarization, Q&A, when prompted. While being able to provide very good answers to questions about information that they were trained with, they tend to hallucinate when the topic is about information that they do \"not know\", i.e. was not included in their training data. Retrieval Augmented Generation combines external resources with LLMs. The main two components of a RAG are therefore a retriever and a generator.  \n",
        "\n",
        "The retriever part can be described as a system that is able to encode our data so that can be easily retrieved the relevant parts of it upon queriying it. The encoding is done using text embeddings, i.e. a model trained to create a vector representation of the information. The best option for implementing a retriever is a vector database. As vector database, there are multiple options, both open source or commercial products. Few examples are ChromaDB, Mevius, FAISS, Pinecone, Weaviate. Our option in this Notebook will be a local instance of ChromaDB (persistent).\n",
        "\n",
        "For the generator part, the obvious option is a LLM. In this Notebook we will use a quantized LLaMA v2 model, from the Kaggle Models collection.  \n",
        "\n",
        "The orchestration of the retriever and generator will be done using Langchain. A specialized function from Langchain allows us to create the receiver-generator in one line of code.\n",
        "\n",
        "## More about this  \n",
        "\n",
        "Do you want to learn more? Look into the `References` section for blog posts and in `More work on the same topic` for Notebooks about the technologies used here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deea9c35",
      "metadata": {
        "papermill": {
          "duration": 0.008483,
          "end_time": "2023-10-27T20:56:19.608601",
          "exception": false,
          "start_time": "2023-10-27T20:56:19.600118",
          "status": "completed"
        },
        "tags": [],
        "id": "deea9c35"
      },
      "source": [
        "# Installations, imports, utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f633640",
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T20:56:19.627014Z",
          "iopub.status.busy": "2023-10-27T20:56:19.626657Z",
          "iopub.status.idle": "2023-10-27T20:59:21.477666Z",
          "shell.execute_reply": "2023-10-27T20:59:21.476526Z"
        },
        "papermill": {
          "duration": 181.863041,
          "end_time": "2023-10-27T20:59:21.480141",
          "exception": false,
          "start_time": "2023-10-27T20:56:19.617100",
          "status": "completed"
        },
        "tags": [],
        "id": "4f633640",
        "outputId": "476c4f62-9c26-4311-9372-f7b2553b310f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==4.33.0 in /opt/conda/lib/python3.10/site-packages (4.33.0)\r\n",
            "Requirement already satisfied: accelerate==0.22.0 in /opt/conda/lib/python3.10/site-packages (0.22.0)\r\n",
            "Collecting einops==0.6.1\r\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting langchain==0.0.300\r\n",
            "  Downloading langchain-0.0.300-py3-none-any.whl (1.7 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting xformers==0.0.21\r\n",
            "  Downloading xformers-0.0.21-cp310-cp310-manylinux2014_x86_64.whl (167.0 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.0/167.0 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting bitsandbytes==0.41.1\r\n",
            "  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting sentence_transformers==2.2.2\r\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
            "\u001b[?25hCollecting chromadb==0.4.12\r\n",
            "  Downloading chromadb-0.4.12-py3-none-any.whl (426 kB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.5/426.5 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (3.12.2)\r\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.16.4)\r\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (1.23.5)\r\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (21.3)\r\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (6.0)\r\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (2023.6.3)\r\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (2.31.0)\r\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.13.3)\r\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.3.3)\r\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (4.66.1)\r\n",
            "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (5.9.3)\r\n",
            "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (2.0.0)\r\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (2.0.17)\r\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (3.8.4)\r\n",
            "Requirement already satisfied: anyio<4.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (3.7.0)\r\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (4.0.2)\r\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (0.6.0)\r\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.300)\r\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\r\n",
            "Collecting langsmith<0.1.0,>=0.0.38 (from langchain==0.0.300)\r\n",
            "  Downloading langsmith-0.0.52-py3-none-any.whl (43 kB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (2.8.5)\r\n",
            "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (1.10.9)\r\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.300) (8.2.2)\r\n",
            "Collecting torch>=1.10.0 (from accelerate==0.22.0)\r\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.15.1)\r\n",
            "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.2.2)\r\n",
            "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.11.2)\r\n",
            "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (3.2.4)\r\n",
            "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.1.99)\r\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb==0.4.12)\r\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hRequirement already satisfied: fastapi<0.100.0,>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.98.0)\r\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.22.0)\r\n",
            "Collecting posthog>=2.4.0 (from chromadb==0.4.12)\r\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\r\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (4.6.3)\r\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb==0.4.12)\r\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb==0.4.12)\r\n",
            "  Downloading onnxruntime-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting pypika>=0.48.9 (from chromadb==0.4.12)\r\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
            "\u001b[?25hCollecting overrides>=7.3.1 (from chromadb==0.4.12)\r\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\r\n",
            "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (5.12.0)\r\n",
            "Collecting bcrypt>=4.0.1 (from chromadb==0.4.12)\r\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.9.0)\r\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (1.12)\r\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1)\r\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1.2)\r\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch>=1.10.0->accelerate==0.22.0)\r\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch>=1.10.0->accelerate==0.22.0)\r\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch>=1.10.0->accelerate==0.22.0)\r\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch>=1.10.0->accelerate==0.22.0)\r\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch>=1.10.0->accelerate==0.22.0)\r\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch>=1.10.0->accelerate==0.22.0)\r\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch>=1.10.0->accelerate==0.22.0)\r\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch>=1.10.0->accelerate==0.22.0)\r\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch>=1.10.0->accelerate==0.22.0)\r\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch>=1.10.0->accelerate==0.22.0)\r\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch>=1.10.0->accelerate==0.22.0)\r\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch>=1.10.0->accelerate==0.22.0)\r\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (68.0.0)\r\n",
            "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (0.40.0)\r\n",
            "Collecting cmake (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0)\r\n",
            "  Downloading cmake-3.27.7-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting lit (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0)\r\n",
            "  Downloading lit-17.0.3.tar.gz (154 kB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (23.1.0)\r\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (3.1.0)\r\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (6.0.4)\r\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.9.2)\r\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.3)\r\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.1)\r\n",
            "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (3.4)\r\n",
            "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.3.0)\r\n",
            "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.1.1)\r\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (3.20.1)\r\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (0.9.0)\r\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.12) (0.27.0)\r\n",
            "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (2023.9.0)\r\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.300) (2.0)\r\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.12)\r\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (23.5.26)\r\n",
            "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (3.20.3)\r\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.33.0) (3.0.9)\r\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.16.0)\r\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.4.12)\r\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\r\n",
            "Requirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.2.1)\r\n",
            "Requirement already satisfied: python-dateutil>2.1 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.8.2)\r\n",
            "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb==0.4.12) (2023.7.22)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0) (1.26.15)\r\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.300) (2.0.2)\r\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.12) (8.1.7)\r\n",
            "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.14.0)\r\n",
            "Requirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.6.0)\r\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (1.0.0)\r\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.17.0)\r\n",
            "Requirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.20.0)\r\n",
            "Requirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (11.0.3)\r\n",
            "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.2.2) (1.3.2)\r\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.2.2) (3.1.0)\r\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers==2.2.2) (9.5.0)\r\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (1.0.0)\r\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.12)\r\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0) (2.1.3)\r\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.22.0) (1.3.0)\r\n",
            "Building wheels for collected packages: sentence_transformers, pypika, lit\r\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
            "\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=7cee6bac5e89ee1673c85dab509e008a2a63fd0ee08333a5c20779f13b9c9f02\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\r\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
            "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=21bc03bab0c7f9666665cebafd6ac3691a8850712ef2dbf98a842db83d8d0eb7\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\r\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
            "\u001b[?25h  Created wheel for lit: filename=lit-17.0.3-py3-none-any.whl size=93257 sha256=341df96664f70d5f8713e6b22d5360f6ed03418f9e1b3297877cfa6f31d080a0\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/b8/42/f6f56aba870f9f3cc895b2e0c970ececaafc7d191217fa10a4\r\n",
            "Successfully built sentence_transformers pypika lit\r\n",
            "Installing collected packages: pypika, monotonic, lit, cmake, bitsandbytes, pulsar-client, overrides, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, jsonpatch, humanfriendly, einops, chroma-hnswlib, bcrypt, posthog, nvidia-cusolver-cu11, nvidia-cudnn-cu11, langsmith, coloredlogs, onnxruntime, langchain, chromadb, triton, torch, xformers, sentence_transformers\r\n",
            "  Attempting uninstall: overrides\r\n",
            "    Found existing installation: overrides 6.5.0\r\n",
            "    Uninstalling overrides-6.5.0:\r\n",
            "      Successfully uninstalled overrides-6.5.0\r\n",
            "  Attempting uninstall: jsonpatch\r\n",
            "    Found existing installation: jsonpatch 1.32\r\n",
            "    Uninstalling jsonpatch-1.32:\r\n",
            "      Successfully uninstalled jsonpatch-1.32\r\n",
            "  Attempting uninstall: torch\r\n",
            "    Found existing installation: torch 2.0.0\r\n",
            "    Uninstalling torch-2.0.0:\r\n",
            "      Successfully uninstalled torch-2.0.0\r\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
            "google-cloud-pubsublite 1.8.2 requires overrides<7.0.0,>=6.0.1, but you have overrides 7.4.0 which is incompatible.\r\n",
            "jupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
            "\u001b[0mSuccessfully installed bcrypt-4.0.1 bitsandbytes-0.41.1 chroma-hnswlib-0.7.3 chromadb-0.4.12 cmake-3.27.7 coloredlogs-15.0.1 einops-0.6.1 humanfriendly-10.0 jsonpatch-1.33 langchain-0.0.300 langsmith-0.0.52 lit-17.0.3 monotonic-1.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 onnxruntime-1.16.1 overrides-7.3.1 posthog-3.0.2 pulsar-client-3.3.0 pypika-0.48.9 sentence_transformers-2.2.2 torch-2.0.1 triton-2.0.0 xformers-0.0.21\r\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\n",
        "bitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cd3737d",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T20:59:21.624603Z",
          "iopub.status.busy": "2023-10-27T20:59:21.624219Z",
          "iopub.status.idle": "2023-10-27T20:59:29.347292Z",
          "shell.execute_reply": "2023-10-27T20:59:29.346135Z"
        },
        "papermill": {
          "duration": 7.79784,
          "end_time": "2023-10-27T20:59:29.350209",
          "exception": false,
          "start_time": "2023-10-27T20:59:21.552369",
          "status": "completed"
        },
        "tags": [],
        "id": "9cd3737d"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from time import time\n",
        "#import chromadb\n",
        "#from chromadb.config import Settings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc807934",
      "metadata": {
        "papermill": {
          "duration": 0.069963,
          "end_time": "2023-10-27T20:59:29.491611",
          "exception": false,
          "start_time": "2023-10-27T20:59:29.421648",
          "status": "completed"
        },
        "tags": [],
        "id": "dc807934"
      },
      "source": [
        "# Initialize model, tokenizer, query pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac27581",
      "metadata": {
        "papermill": {
          "duration": 0.070033,
          "end_time": "2023-10-27T20:59:29.632562",
          "exception": false,
          "start_time": "2023-10-27T20:59:29.562529",
          "status": "completed"
        },
        "tags": [],
        "id": "cac27581"
      },
      "source": [
        "Define the model, the device, and the `bitsandbytes` configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c42ecd",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T20:59:29.774721Z",
          "iopub.status.busy": "2023-10-27T20:59:29.773919Z",
          "iopub.status.idle": "2023-10-27T20:59:29.864243Z",
          "shell.execute_reply": "2023-10-27T20:59:29.863458Z"
        },
        "papermill": {
          "duration": 0.163665,
          "end_time": "2023-10-27T20:59:29.866257",
          "exception": false,
          "start_time": "2023-10-27T20:59:29.702592",
          "status": "completed"
        },
        "tags": [],
        "id": "04c42ecd"
      },
      "outputs": [],
      "source": [
        "model_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33296d38",
      "metadata": {
        "papermill": {
          "duration": 0.070293,
          "end_time": "2023-10-27T20:59:30.007969",
          "exception": false,
          "start_time": "2023-10-27T20:59:29.937676",
          "status": "completed"
        },
        "tags": [],
        "id": "33296d38"
      },
      "source": [
        "Prepare the model and the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "489fbbf7",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T20:59:30.152371Z",
          "iopub.status.busy": "2023-10-27T20:59:30.151551Z",
          "iopub.status.idle": "2023-10-27T21:02:57.429735Z",
          "shell.execute_reply": "2023-10-27T21:02:57.428570Z"
        },
        "papermill": {
          "duration": 207.351985,
          "end_time": "2023-10-27T21:02:57.431750",
          "exception": false,
          "start_time": "2023-10-27T20:59:30.079765",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "cec449360a1b41f183b430322a8a9f21"
          ]
        },
        "id": "489fbbf7",
        "outputId": "1d8174c3-706a-4952-9561-a2adc04ed6f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cec449360a1b41f183b430322a8a9f21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepare model, tokenizer: 207.272 sec.\n"
          ]
        }
      ],
      "source": [
        "time_1 = time()\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        ")\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "time_2 = time()\n",
        "print(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21c5a616",
      "metadata": {
        "papermill": {
          "duration": 0.071483,
          "end_time": "2023-10-27T21:02:57.575572",
          "exception": false,
          "start_time": "2023-10-27T21:02:57.504089",
          "status": "completed"
        },
        "tags": [],
        "id": "21c5a616"
      },
      "source": [
        "Define the query pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3abb7c2",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:02:57.718925Z",
          "iopub.status.busy": "2023-10-27T21:02:57.718554Z",
          "iopub.status.idle": "2023-10-27T21:02:59.493954Z",
          "shell.execute_reply": "2023-10-27T21:02:59.492866Z"
        },
        "papermill": {
          "duration": 1.849478,
          "end_time": "2023-10-27T21:02:59.496057",
          "exception": false,
          "start_time": "2023-10-27T21:02:57.646579",
          "status": "completed"
        },
        "tags": [],
        "id": "d3abb7c2",
        "outputId": "6827cc76-a3d7-4306-f20e-7521856f2b73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepare pipeline: 1.77 sec.\n"
          ]
        }
      ],
      "source": [
        "time_1 = time()\n",
        "query_pipeline = transformers.pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",)\n",
        "time_2 = time()\n",
        "print(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e6d4257",
      "metadata": {
        "papermill": {
          "duration": 0.07169,
          "end_time": "2023-10-27T21:02:59.639272",
          "exception": false,
          "start_time": "2023-10-27T21:02:59.567582",
          "status": "completed"
        },
        "tags": [],
        "id": "0e6d4257"
      },
      "source": [
        "We define a function for testing the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43f4b1c6",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:02:59.783315Z",
          "iopub.status.busy": "2023-10-27T21:02:59.782571Z",
          "iopub.status.idle": "2023-10-27T21:02:59.788716Z",
          "shell.execute_reply": "2023-10-27T21:02:59.787817Z"
        },
        "papermill": {
          "duration": 0.080139,
          "end_time": "2023-10-27T21:02:59.790639",
          "exception": false,
          "start_time": "2023-10-27T21:02:59.710500",
          "status": "completed"
        },
        "tags": [],
        "id": "43f4b1c6"
      },
      "outputs": [],
      "source": [
        "def test_model(tokenizer, pipeline, prompt_to_test):\n",
        "    \"\"\"\n",
        "    Perform a query\n",
        "    print the result\n",
        "    Args:\n",
        "        tokenizer: the tokenizer\n",
        "        pipeline: the pipeline\n",
        "        prompt_to_test: the prompt\n",
        "    Returns\n",
        "        None\n",
        "    \"\"\"\n",
        "    # adapted from https://huggingface.co/blog/llama2#using-transformers\n",
        "    time_1 = time()\n",
        "    sequences = pipeline(\n",
        "        prompt_to_test,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=200,)\n",
        "    time_2 = time()\n",
        "    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n",
        "    for seq in sequences:\n",
        "        print(f\"Result: {seq['generated_text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2444aca8",
      "metadata": {
        "papermill": {
          "duration": 0.070941,
          "end_time": "2023-10-27T21:02:59.932300",
          "exception": false,
          "start_time": "2023-10-27T21:02:59.861359",
          "status": "completed"
        },
        "tags": [],
        "id": "2444aca8"
      },
      "source": [
        "## Test the query pipeline\n",
        "\n",
        "We test the pipeline with a query about the meaning of State of the Union (SOTU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c565a85",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:00.077578Z",
          "iopub.status.busy": "2023-10-27T21:03:00.076738Z",
          "iopub.status.idle": "2023-10-27T21:03:07.197627Z",
          "shell.execute_reply": "2023-10-27T21:03:07.196596Z"
        },
        "papermill": {
          "duration": 7.196414,
          "end_time": "2023-10-27T21:03:07.199767",
          "exception": false,
          "start_time": "2023-10-27T21:03:00.003353",
          "status": "completed"
        },
        "tags": [],
        "id": "8c565a85",
        "outputId": "f409dbe7-c9e5-43b6-e160-eac435f70622"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test inference: 7.116 sec.\n",
            "Result: Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\n",
            "The State of the Union address is an annual speech given by the President of the United States to a joint session of Congress. It provides an update on the President's policies, accomplishments, and priorities, as well as a call to action for the legislative branch to work together to address pressing national issues.\n"
          ]
        }
      ],
      "source": [
        "test_model(tokenizer,\n",
        "           query_pipeline,\n",
        "           \"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8afac76",
      "metadata": {
        "papermill": {
          "duration": 0.0722,
          "end_time": "2023-10-27T21:03:07.344434",
          "exception": false,
          "start_time": "2023-10-27T21:03:07.272234",
          "status": "completed"
        },
        "tags": [],
        "id": "d8afac76"
      },
      "source": [
        "# Retrieval Augmented Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8569323",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-23T19:22:16.434937Z",
          "iopub.status.busy": "2023-09-23T19:22:16.433666Z",
          "iopub.status.idle": "2023-09-23T19:22:16.440864Z",
          "shell.execute_reply": "2023-09-23T19:22:16.439217Z",
          "shell.execute_reply.started": "2023-09-23T19:22:16.434891Z"
        },
        "papermill": {
          "duration": 0.071285,
          "end_time": "2023-10-27T21:03:07.488216",
          "exception": false,
          "start_time": "2023-10-27T21:03:07.416931",
          "status": "completed"
        },
        "tags": [],
        "id": "a8569323"
      },
      "source": [
        "## Check the model with a HuggingFace pipeline\n",
        "\n",
        "\n",
        "We check the model with a HF pipeline, using a query about the meaning of State of the Union (SOTU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a55c51a2",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:07.632834Z",
          "iopub.status.busy": "2023-10-27T21:03:07.632222Z",
          "iopub.status.idle": "2023-10-27T21:03:12.199600Z",
          "shell.execute_reply": "2023-10-27T21:03:12.198697Z"
        },
        "papermill": {
          "duration": 4.641707,
          "end_time": "2023-10-27T21:03:12.201731",
          "exception": false,
          "start_time": "2023-10-27T21:03:07.560024",
          "status": "completed"
        },
        "tags": [],
        "id": "a55c51a2",
        "outputId": "11c44994-0995-4adc-d7a1-022fe8fc6dfd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nThe State of the Union address is an annual speech given by the President of the United States to a joint session of Congress, in which the President reports on the current state of the union and outlines their legislative agenda for the upcoming year.'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
        "# checking again that everything is working fine\n",
        "llm(prompt=\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8951a9e",
      "metadata": {
        "papermill": {
          "duration": 0.071941,
          "end_time": "2023-10-27T21:03:12.347303",
          "exception": false,
          "start_time": "2023-10-27T21:03:12.275362",
          "status": "completed"
        },
        "tags": [],
        "id": "f8951a9e"
      },
      "source": [
        "## Ingestion of data using Text loder\n",
        "\n",
        "We will ingest the newest presidential address, from Jan 2023."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f45938da",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:12.492486Z",
          "iopub.status.busy": "2023-10-27T21:03:12.491830Z",
          "iopub.status.idle": "2023-10-27T21:03:12.510111Z",
          "shell.execute_reply": "2023-10-27T21:03:12.509378Z"
        },
        "papermill": {
          "duration": 0.092791,
          "end_time": "2023-10-27T21:03:12.511927",
          "exception": false,
          "start_time": "2023-10-27T21:03:12.419136",
          "status": "completed"
        },
        "tags": [],
        "id": "f45938da"
      },
      "outputs": [],
      "source": [
        "loader = TextLoader(\"/kaggle/input/president-bidens-state-of-the-union-2023/biden-sotu-2023-planned-official.txt\",\n",
        "                    encoding=\"utf8\")\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31db8500",
      "metadata": {
        "papermill": {
          "duration": 0.071305,
          "end_time": "2023-10-27T21:03:12.654717",
          "exception": false,
          "start_time": "2023-10-27T21:03:12.583412",
          "status": "completed"
        },
        "tags": [],
        "id": "31db8500"
      },
      "source": [
        "## Split data in chunks\n",
        "\n",
        "We split data in chunks using a recursive character text splitter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3636cbf3",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:12.799624Z",
          "iopub.status.busy": "2023-10-27T21:03:12.798854Z",
          "iopub.status.idle": "2023-10-27T21:03:12.826146Z",
          "shell.execute_reply": "2023-10-27T21:03:12.825120Z"
        },
        "papermill": {
          "duration": 0.102636,
          "end_time": "2023-10-27T21:03:12.828350",
          "exception": false,
          "start_time": "2023-10-27T21:03:12.725714",
          "status": "completed"
        },
        "tags": [],
        "id": "3636cbf3"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddc41386",
      "metadata": {
        "papermill": {
          "duration": 0.071654,
          "end_time": "2023-10-27T21:03:12.973609",
          "exception": false,
          "start_time": "2023-10-27T21:03:12.901955",
          "status": "completed"
        },
        "tags": [],
        "id": "ddc41386"
      },
      "source": [
        "## Creating Embeddings and Storing in Vector Store"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b1cef9c",
      "metadata": {
        "papermill": {
          "duration": 0.072129,
          "end_time": "2023-10-27T21:03:13.117787",
          "exception": false,
          "start_time": "2023-10-27T21:03:13.045658",
          "status": "completed"
        },
        "tags": [],
        "id": "8b1cef9c"
      },
      "source": [
        "Create the embeddings using Sentence Transformer and HuggingFace embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8fe608f",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:13.262680Z",
          "iopub.status.busy": "2023-10-27T21:03:13.261674Z",
          "iopub.status.idle": "2023-10-27T21:03:20.269116Z",
          "shell.execute_reply": "2023-10-27T21:03:20.268254Z"
        },
        "papermill": {
          "duration": 7.082713,
          "end_time": "2023-10-27T21:03:20.271413",
          "exception": false,
          "start_time": "2023-10-27T21:03:13.188700",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "d1a3658f4b024df3bf6576b1ee87a9a7",
            "a90809940ca1443680aa88b38cd737a2",
            "083427843cbb440182b66b04bb258ae9",
            "11bcd3afd3b14fb6be755a347e45006c",
            "cdf11c758d834188bb8bab998140812a",
            "a95874e8838149ab8ea55b2a69b8c638",
            "1cd30097e2594f31a82716410603bff0",
            "27215076d17b43f8b9d1eebfbe53c475",
            "2098a6790ba244c7bc3a5cb5219faa5b",
            "18c9d2ae4c15430ea457e77409dc0131",
            "0a50b2886b1940d2939ec215dd683d36",
            "c93cddfea5f34656bd58103507dd70de",
            "2cd80a35e0c444f78ed8c90c621e3984",
            "fc05cb724d254a4b8b01b76253f1dd17"
          ]
        },
        "id": "a8fe608f",
        "outputId": "939c35e7-92c7-420b-83ab-3aacc713d799"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1a3658f4b024df3bf6576b1ee87a9a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)a8e1d/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a90809940ca1443680aa88b38cd737a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "083427843cbb440182b66b04bb258ae9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)b20bca8e1d/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11bcd3afd3b14fb6be755a347e45006c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)0bca8e1d/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdf11c758d834188bb8bab998140812a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a95874e8838149ab8ea55b2a69b8c638",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)e1d/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1cd30097e2594f31a82716410603bff0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27215076d17b43f8b9d1eebfbe53c475",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2098a6790ba244c7bc3a5cb5219faa5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18c9d2ae4c15430ea457e77409dc0131",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)a8e1d/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a50b2886b1940d2939ec215dd683d36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c93cddfea5f34656bd58103507dd70de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)8e1d/train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2cd80a35e0c444f78ed8c90c621e3984",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)b20bca8e1d/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc05cb724d254a4b8b01b76253f1dd17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)bca8e1d/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0163fe3",
      "metadata": {
        "papermill": {
          "duration": 0.07346,
          "end_time": "2023-10-27T21:03:20.419320",
          "exception": false,
          "start_time": "2023-10-27T21:03:20.345860",
          "status": "completed"
        },
        "tags": [],
        "id": "e0163fe3"
      },
      "source": [
        "Initialize ChromaDB with the document splits, the embeddings defined previously and with the option to persist it locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d44d4386",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:20.569088Z",
          "iopub.status.busy": "2023-10-27T21:03:20.568404Z",
          "iopub.status.idle": "2023-10-27T21:03:21.695319Z",
          "shell.execute_reply": "2023-10-27T21:03:21.694514Z"
        },
        "papermill": {
          "duration": 1.204428,
          "end_time": "2023-10-27T21:03:21.697360",
          "exception": false,
          "start_time": "2023-10-27T21:03:20.492932",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "c56767445a5f426b9669c82bc450d6be"
          ]
        },
        "id": "d44d4386",
        "outputId": "9902bfa6-7abe-4153-a1ee-8c1933e7a4c2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c56767445a5f426b9669c82bc450d6be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ede70a2d",
      "metadata": {
        "papermill": {
          "duration": 0.073547,
          "end_time": "2023-10-27T21:03:21.846037",
          "exception": false,
          "start_time": "2023-10-27T21:03:21.772490",
          "status": "completed"
        },
        "tags": [],
        "id": "ede70a2d"
      },
      "source": [
        "## Initialize chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12a89658",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:21.995328Z",
          "iopub.status.busy": "2023-10-27T21:03:21.994982Z",
          "iopub.status.idle": "2023-10-27T21:03:22.000248Z",
          "shell.execute_reply": "2023-10-27T21:03:21.999404Z"
        },
        "papermill": {
          "duration": 0.082483,
          "end_time": "2023-10-27T21:03:22.002212",
          "exception": false,
          "start_time": "2023-10-27T21:03:21.919729",
          "status": "completed"
        },
        "tags": [],
        "id": "12a89658"
      },
      "outputs": [],
      "source": [
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e171a1d",
      "metadata": {
        "papermill": {
          "duration": 0.075491,
          "end_time": "2023-10-27T21:03:22.151652",
          "exception": false,
          "start_time": "2023-10-27T21:03:22.076161",
          "status": "completed"
        },
        "tags": [],
        "id": "1e171a1d"
      },
      "source": [
        "## Test the Retrieval-Augmented Generation\n",
        "\n",
        "\n",
        "We define a test function, that will run the query and time it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2863b132",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:22.303074Z",
          "iopub.status.busy": "2023-10-27T21:03:22.302237Z",
          "iopub.status.idle": "2023-10-27T21:03:22.307653Z",
          "shell.execute_reply": "2023-10-27T21:03:22.306729Z"
        },
        "papermill": {
          "duration": 0.083507,
          "end_time": "2023-10-27T21:03:22.309806",
          "exception": false,
          "start_time": "2023-10-27T21:03:22.226299",
          "status": "completed"
        },
        "tags": [],
        "id": "2863b132"
      },
      "outputs": [],
      "source": [
        "def test_rag(qa, query):\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    time_1 = time()\n",
        "    result = qa.run(query)\n",
        "    time_2 = time()\n",
        "    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n",
        "    print(\"\\nResult: \", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efb9fc42",
      "metadata": {
        "papermill": {
          "duration": 0.074358,
          "end_time": "2023-10-27T21:03:22.460824",
          "exception": false,
          "start_time": "2023-10-27T21:03:22.386466",
          "status": "completed"
        },
        "tags": [],
        "id": "efb9fc42"
      },
      "source": [
        "Let's check few queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1b1f2ff",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:22.611036Z",
          "iopub.status.busy": "2023-10-27T21:03:22.610650Z",
          "iopub.status.idle": "2023-10-27T21:03:34.578891Z",
          "shell.execute_reply": "2023-10-27T21:03:34.577954Z"
        },
        "papermill": {
          "duration": 12.045607,
          "end_time": "2023-10-27T21:03:34.581176",
          "exception": false,
          "start_time": "2023-10-27T21:03:22.535569",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "eebb150564124f15b30457757009c9e7"
          ]
        },
        "id": "c1b1f2ff",
        "outputId": "f5a5a2af-0d38-47dd-fcd5-61167f92463b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eebb150564124f15b30457757009c9e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Inference time: 11.964 sec.\n",
            "\n",
            "Result:   The State of the Union in 2023 focused on several key topics, including the nation's economic strength, the competition with China, and the need to come together as a nation to face the challenges ahead. The President emphasized the importance of American innovation, industries, and military modernization to ensure the country's safety and stability. The President also highlighted the nation's resilience and optimism, urging Americans to see each other as fellow citizens and to work together to overcome the challenges facing the country.\n"
          ]
        }
      ],
      "source": [
        "query = \"What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\"\n",
        "test_rag(qa, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b764d2f",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:34.730684Z",
          "iopub.status.busy": "2023-10-27T21:03:34.730387Z",
          "iopub.status.idle": "2023-10-27T21:03:45.642067Z",
          "shell.execute_reply": "2023-10-27T21:03:45.641090Z"
        },
        "papermill": {
          "duration": 10.988249,
          "end_time": "2023-10-27T21:03:45.644257",
          "exception": false,
          "start_time": "2023-10-27T21:03:34.656008",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "fa707ac8283b42f5a3fc38bffca9725b"
          ]
        },
        "id": "7b764d2f",
        "outputId": "4561ff36-d2a0-483d-ca3b-9a6579307131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What is the nation economic status? Summarize. Keep it under 200 words.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa707ac8283b42f5a3fc38bffca9725b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Inference time: 10.907 sec.\n",
            "\n",
            "Result:   The nation's economic status is strong, with a low unemployment rate of 3.4%, near record lows for Black and Hispanic workers, and fastest growth in 40 years in manufacturing jobs. The president highlights the progress made in creating good-paying jobs, exporting American products, and reducing inflation. However, the president acknowledges there is still more work to be done to fully recover from the pandemic and Putin's war.\n"
          ]
        }
      ],
      "source": [
        "query = \"What is the nation economic status? Summarize. Keep it under 200 words.\"\n",
        "test_rag(qa, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6b8ee84",
      "metadata": {
        "papermill": {
          "duration": 0.076435,
          "end_time": "2023-10-27T21:03:45.796935",
          "exception": false,
          "start_time": "2023-10-27T21:03:45.720500",
          "status": "completed"
        },
        "tags": [],
        "id": "a6b8ee84"
      },
      "source": [
        "## Document sources\n",
        "\n",
        "Let's check the documents sources, for the last query run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf90c12d",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:45.950514Z",
          "iopub.status.busy": "2023-10-27T21:03:45.950160Z",
          "iopub.status.idle": "2023-10-27T21:03:45.993003Z",
          "shell.execute_reply": "2023-10-27T21:03:45.991579Z"
        },
        "papermill": {
          "duration": 0.122251,
          "end_time": "2023-10-27T21:03:45.994982",
          "exception": false,
          "start_time": "2023-10-27T21:03:45.872731",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "59a4f11809a74e758f49c9617a81c038"
          ]
        },
        "id": "bf90c12d",
        "outputId": "29c2a853-5e6d-45be-813a-aa7bdd4d8cac"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59a4f11809a74e758f49c9617a81c038",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What is the nation economic status? Summarize. Keep it under 200 words.\n",
            "Retrieved documents: 4\n",
            "Source:  /kaggle/input/president-bidens-state-of-the-union-2023/biden-sotu-2023-planned-official.txt\n",
            "Text:  forward. Of never giving up. A story that is unique among all nations. We are the only country that has emerged from every crisis stronger than when we entered it. That is what we are doing again. Two years ago, our economy was reeling. As I stand here tonight, we have created a record 12 million new jobs, more jobs created in two years than any president has ever created in four years. Two years ago, COVID had shut down our businesses, closed our schools, and robbed us of so much. Today, COVID no longer controls our lives. And two years ago, our democracy faced its greatest threat since the Civil War. Today, though bruised, our democracy remains unbowed and unbroken. As we gather here tonight, we are writing the next chapter in the great American story, a story of progress and resilience. When world leaders ask me to define America, I define our country in one word: Possibilities. You know, we’re often told that Democrats and Republicans can’t work together. But over these past two \n",
            "\n",
            "Source:  /kaggle/input/president-bidens-state-of-the-union-2023/biden-sotu-2023-planned-official.txt\n",
            "Text:  on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops. \n",
            "\n",
            "Source:  /kaggle/input/president-bidens-state-of-the-union-2023/biden-sotu-2023-planned-official.txt\n",
            "Text:  Americans, we meet tonight at an inflection point. One of those moments that only a few generations ever face, where the decisions we make now will decide the course of this nation and of the world for decades to come. We are not bystanders to history. We are not powerless before the forces that confront us. It is within our power, of We the People. We are facing the test of our time and the time for choosing is at hand. We must be the nation we have always been at our best. Optimistic. Hopeful. Forward-looking. A nation that embraces, light over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the \n",
            "\n",
            "Source:  /kaggle/input/president-bidens-state-of-the-union-2023/biden-sotu-2023-planned-official.txt\n",
            "Text:  about being able to look your kid in the eye and say, “Honey –it’s going to be OK,” and mean it. So, let’s look at the results. Unemployment rate at 3.4%, a 50-year low. Near record low unemployment for Black and Hispanic workers. We’ve already created 800,000 good-paying manufacturing jobs, the fastest growth in 40 years. Where is it written that America can’t lead the world in manufacturing again? For too many decades, we imported products and exported jobs. Now, thanks to all we’ve done, we’re exporting American products and creating American jobs. Inflation has been a global problem because of the pandemic that disrupted supply chains and Putin’s war that disrupted energy and food supplies. But we’re better positioned than any country on Earth. We have more to do, but here at home, inflation is coming down. Here at home, gas prices are down $1.50 a gallon since their peak. Food inflation is coming down. Inflation has fallen every month for the last six months while take home pay \n",
            "\n"
          ]
        }
      ],
      "source": [
        "docs = vectordb.similarity_search(query)\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Retrieved documents: {len(docs)}\")\n",
        "for doc in docs:\n",
        "    doc_details = doc.to_json()['kwargs']\n",
        "    print(\"Source: \", doc_details['metadata']['source'])\n",
        "    print(\"Text: \", doc_details['page_content'], \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 453.685483,
      "end_time": "2023-10-27T21:03:49.973763",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-10-27T20:56:16.288280",
      "version": "2.4.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}